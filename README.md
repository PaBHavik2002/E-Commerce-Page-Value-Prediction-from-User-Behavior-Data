
# E-Commerce-Page-Value-Prediction-from-User-Behavior-Data

> This project focuses on predicting the Page Value of e-commerce website sessions using user clickstream and engagement data. Page Value measures the average monetary value generated by a page, helping online retailers identify which pages contribute most to conversions and revenue.

> The dataset contains 12,330 sessions with features capturing different types of page visits and durations, including:
	
 	Administrative & Informational Pages – number of visits and total time spent.
	Product-Related Pages – key engagement measure for purchase behavior.
	Bounce Rates & Exit Rates – indicators of session quality.
	Page Values – the target variable for prediction.
	SpecialDay, Month, VisitorType, Weekend – contextual factors around the session.

> After exploratory data analysis, I engineered new features to better capture user behavior patterns:

	Ratios of page types to total pages viewed.
	Time distribution between page categories.
	Engagement metrics combining duration, value, and bounce rates.
	Seasonal variables derived from month data.
	Flags for high-value visitors and weekend high-engagement sessions.

>  Evaluated Multiple Regression model to predict -> Target Variable (Continous Variable)

	Linear Model -> Baseline Model For Interpretability
 	Polynomial Regression -> Capturing Non-linear Relationship
	Lasso Regularization (L1) -> For Feature Selection and reducing noise
 	Random Forest Regression -> Ensemble Approach for non-linearity
	Gradient Boost Regression -> Boosting Method for Improved Accuracy

   ```python
  	# All These Wrapped In Function
   def train_model(xtrain, xtest, ytest, ytrain):
	-
 	-
	-
 	-
	return data

```

Method Used For Evaluation
> RMSE (Root Mean Squared)

 This evaluation helped understand the model performance as all the models we have is regression and thus we need something to evaluate based on the error between the actual and predicted.
 The reason why this metric used over other liker Mean Squared Error (mse) and Mean Absolute Error (MAE), is beacause of the nature of RMSE, takes the square root of MSE, so the result is in the same units as the target variable. This makes it easier to interpret — you can directly say, “on average, predictions are X units off.”

> R2 (Coefficient of Determination)

 This is used to check, how effective our model is in capturing the variance in the data. It is measured between 0 to 1, where 0 being the worst and 1 being good model.

> What's New

 	1. Feature Engineering
  	2. Multiple Models Comparison
   	3. Basline Model Check
	4. Training and Testing Score Visualization
 	5. Model's Accuracy Demonstration VIA visuals

> Data Pipeline for scaling the features
```python
	def convert_Xtrain(data_var):

    # Saving encoders for specific columns
    lbEncoder = {}
    scEncoder = {}
    prEncoder = {}

    if not data_var.empty:

        catData = data_var.select_dtypes(include=['bool', 'object'])
        numData = data_var.select_dtypes(include=['int', 'float'])

        # First Categorical Data
        catCol = catData.columns

        for eachColumn in catCol:

            # Fitting the method
            encoder = LabelEncoder()
            new_Column_Value = encoder.fit_transform(catData[eachColumn])
            lbEncoder[eachColumn] = encoder

            # Scaling the encoded ones
            scaler = StandardScaler()
            scaled_New_values = scaler.fit_transform(pandas.DataFrame(new_Column_Value, columns=[eachColumn]))
            scEncoder[eachColumn] = scaler

            # Passing it to the respective column
            catData[eachColumn] = scaled_New_values.flatten()

            # Transferring each transformers

        # Second Continous Data
        numCol = numData.columns

        for eachColumns in numCol:

            # Power Transformer
            columnMean = numData[eachColumns].mean()
            columnMedian = numData[eachColumns].median()

            # This transformer handles the skewness of the data as well as
            # It scales down the columns. So no need to use any transformation method
            if columnMean > columnMedian:
                
                # When the data is right skewed
                transformer = PowerTransformer(method='yeo-johnson')
                numScaledValues = transformer.fit_transform(pandas.DataFrame(numData[eachColumns], columns=[eachColumns]))
                numData[eachColumns] = numScaledValues
            
            elif columnMean < columnMedian:

                # When the data is left skewed
                transformer = PowerTransformer(method='yeo-johnson')
                numScaledValues = transformer.fit_transform(pandas.DataFrame(numData[eachColumns], columns=[eachColumns]))
                numData[eachColumns] = numScaledValues
            

            # Transferring the transformer
            prEncoder[eachColumns] = transformer

        # Concatenating both the data
        concat_data = pandas.DataFrame(
            pandas.concat([catData, numData], axis=1)
        )

        return concat_data, lbEncoder, scEncoder, prEncoder

    else:
        return 'please pass the data in this function'
```

> Model performance was assessed using Root Mean Squared Error (RMSE) and R² on both training and test data. RMSE values were consistent between training and testing, indicating good generalization. Among all models, Random Forest and Gradient Boosting achieved the best balance of low RMSE and high R², effectively capturing the variance in Page Value while avoiding overfitting.

> The results demonstrate that time spent on product-related pages and session engagement patterns are strong predictors of Page Value. Businesses could leverage such models to prioritize high-value pages, optimize marketing campaigns, and improve site design for revenue impact.

> Tools used include Python, Pandas, NumPy, Matplotlib, Seaborn, and Scikit-learn, with all development carried out in Jupyter Notebook.
