{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "appealing_desc",
   "metadata": {},
   "source": [
    "This notebook predicts customer sentiment and revenue generation based on their browsing behavior. By analyzing various features, we aim to understand the key drivers behind customer decisions and provide actionable insights for the business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "company_context",
   "metadata": {},
   "source": [
    "## Company Context\n",
    "Our company is an e-commerce platform that sells a variety of products online. Understanding customer behavior is crucial for us to enhance user experience, optimize marketing strategies, and ultimately increase revenue. This analysis will help us identify patterns in how customers interact with our website and what leads to a purchase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "business_insights",
   "metadata": {},
   "source": [
    "## Business Insights\n",
    "The primary goal of this analysis is to gain actionable insights into what drives customer sentiment and revenue. We want to answer questions like:\n",
    "* Which marketing channels are most effective?\n",
    "* What is the impact of website performance on customer decisions?\n",
    "* How does browsing behavior differ between new and returning visitors?\n",
    "* What are the characteristics of customers who are most likely to make a purchase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10636edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as mat\n",
    "import seaborn\n",
    "import numpy \n",
    "from matplotlib.gridspec import GridSpec as grid\n",
    "from matplotlib.ticker import MultipleLocator as ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd59d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data\n",
    "data = pandas.read_csv(r\"C:\\\\Users\\\\Bhavik Parmar\\\\OneDrive\\\\Desktop\\\\Data\\\\Shoppers_Behaviour_and_Revenue.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d444c40",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9ef087",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ad451",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_eng",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "To better understand customer behavior, we will create new features from the existing data. This includes:\n",
    "* **Total Pages Viewed:** Combining administrative, informational, and product-related pages to get a sense of overall engagement.\n",
    "* **Total Duration:** Summing up the time spent on different page types to measure session length.\n",
    "* **Engagement Score:** A custom metric that combines session duration, page values, and bounce rates to quantify user engagement.\n",
    "* **Seasonal Analysis:** Converting month data into seasons to identify any seasonal trends in customer behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cf7c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1998cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgExitRate = data['ExitRates'].mean()\n",
    "avgExitRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84769d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Def Function Creation Cell ---> a window to have all the functions used in this whole session\n",
    "\n",
    "def apply_isBounce(row):\n",
    "    if (row['BounceRates'] == 1) & (row['ProductRelated'] == 1) & (row['ExitRates'] == 1):\n",
    "        return 'Yes'\n",
    "    elif (row['BounceRates'] != 1) & (row['ProductRelated'] != 1) & (row['ExitRates'] != 1):\n",
    "        return 'No'\n",
    "    else:\n",
    "        return 'MayBe'\n",
    "    \n",
    "def apply_isExistedReally(row, avgExitRate):\n",
    "\n",
    "    if row['ExitRates'] > avgExitRate:\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "    \n",
    "def apply_hasLongSession(row, avgtotal):\n",
    "    if row['totalDuration'] > avgtotal:\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for feature Engineering --> First function \n",
    "\n",
    "\n",
    "def first_engineering(data):\n",
    "\n",
    "    # Basic Transformation of the columns    \n",
    "    data['totalPagesViewed'] = data['Informational'] + data['Administrative'] + data['ProductRelated']\n",
    "    \n",
    "    data['totalDuration'] = data['Informational_Duration'] + data['Administrative_Duration'] + data['ProductRelated_Duration']\n",
    "    \n",
    "    data['avgTimePerPage'] = data['totalDuration'] / data['totalPagesViewed']\n",
    "\n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = first_engineering(data = data) # Execution of the first function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce3dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second function for data engineering\n",
    "def second_engineering(data):\n",
    "    \n",
    "    avgTotalDuration = data['totalDuration'].mean()\n",
    "    data['hasLongSession'] = data.apply(lambda row: apply_hasLongSession(row, avgTotalDuration), axis=1)\n",
    "\n",
    "    data['productFocus'] = data['ProductRelated'] / data['totalPagesViewed']\n",
    "\n",
    "    data['productTimeRatio'] = data['ProductRelated_Duration'] / data['totalDuration']\n",
    "\n",
    "    \"\"\"Some Flag Based Columns (Categorical)\"\"\"\n",
    "    # Whether They bounced ?\n",
    "    avgExitRate = data['ExitRates'].mean()\n",
    "    data['isBounce'] = data.apply(lambda row: apply_isBounce(row), axis=1)\n",
    "    \n",
    "    data['isExit'] = data.apply(lambda row: apply_isExistedReally(row, avgExitRate), axis=1)\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63233681",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = second_engineering(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5334e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def third_featureEngineering(data):\n",
    "     \n",
    "    # 1. Pages per Category Ratio\n",
    "    data['adminPageRatio'] = data['Administrative'] / data['totalPagesViewed']\n",
    "    data['infoPageRatio'] = data['Informational'] / data['totalPagesViewed']\n",
    "    \n",
    "    # 2. Time Distribution Ratios\n",
    "    data['adminTimeRatio'] = data['Administrative_Duration'] / data['totalDuration']\n",
    "    data['infoTimeRatio'] = data['Informational_Duration'] / data['totalDuration']\n",
    "    \n",
    "    # 3. Interaction Efficiency: Value per Page\n",
    "    data['pageValuePerView'] = data['PageValues'] / (data['totalPagesViewed'] + 1e-6)\n",
    "    \n",
    "    # 4. Engagement Score (Custom metric combining duration, value, and low bounce rate)\n",
    "    data['engagementScore'] = (\n",
    "        (data['totalDuration'] / (data['totalDuration'].max() + 1e-6)) * 0.4 +\n",
    "        (data['PageValues'] / (data['PageValues'].max() + 1e-6)) * 0.4 +\n",
    "        ((1 - data['BounceRates']) / (1 + data['BounceRates'].max())) * 0.2\n",
    "    )\n",
    "    \n",
    "    month_map = {\n",
    "        'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'June': 6,\n",
    "        'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "    }\n",
    "    data['monthNum'] = data['Month'].map(month_map)\n",
    "    \n",
    "    def assign_season(m):\n",
    "        if m in [12, 1, 2]: return 'Winter'\n",
    "        elif m in [3, 4, 5]: return 'Spring'\n",
    "        elif m in [6, 7, 8]: return 'Summer'\n",
    "        else: return 'Fall'\n",
    "    \n",
    "    data['season'] = data['monthNum'].apply(assign_season)\n",
    "    \n",
    "    # 6. High Value Visitor Flag\n",
    "    avg_page_value = data['PageValues'].mean()\n",
    "    data['isHighValueVisitor'] = numpy.where(data['PageValues'] > avg_page_value, 'Yes', 'No')\n",
    "    \n",
    "    # 7. Time to Value Ratio\n",
    "    data['timeToValueRatio'] = data['totalDuration'] / (data['PageValues'] + 1e-6)\n",
    "    \n",
    "    # 8. Weekend + High Engagement Interaction\n",
    "    data['weekendHighEngagement'] = numpy.where(\n",
    "        (data['Weekend'] == True) & (data['engagementScore'] > data['engagementScore'].mean()),\n",
    "        'Yes', 'No'\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24921d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = third_featureEngineering(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41568b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing some missing values ! \n",
    "haslongsessionMode = data['hasLongSession'].mode()[0]\n",
    "data['hasLongSession']= data['hasLongSession'].fillna(haslongsessionMode)\n",
    "productFocusMean = int(data['productFocus'].mean())\n",
    "data.fillna(productFocusMean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf5a66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Nann Values\n",
    "data['productTimeRatio'] = data['productTimeRatio'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b396423",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis ---> Continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e8fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.set_option('display.max_columns', None)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f2195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Related \n",
    "overview = data['ProductRelated'].describe()\n",
    "print(overview)\n",
    "maxVal = data['ProductRelated'].max()\n",
    "print('Max Value', maxVal)\n",
    "\n",
    "# Product Page Visists\n",
    "avg_val = data['ProductRelated'].mean()\n",
    "specific = data.loc[data['ProductRelated'] > avg_val]\n",
    "\n",
    "seaborn.histplot(y=specific['ProductRelated'], x=specific['Month'])\n",
    "\n",
    "meanforEachday = specific.groupby('Month')['ProductRelated'].mean()\n",
    "for i, month in enumerate(meanforEachday.index):\n",
    "    mat.plot(i, meanforEachday[month], marker='o', color='red', label='Mean' if i == 0 else \"\")\n",
    "\n",
    "mat.legend()\n",
    "mat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd1568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting countplot\n",
    "countByMonths = specific.groupby('Month')['ProductRelated'].count()\n",
    "pointer = 300\n",
    "colorSeq = [ 'red' if val > pointer else 'blue' for val in countByMonths.values.tolist() ]\n",
    "\n",
    "# Plotting count plot\n",
    "seaborn.barplot(\n",
    "    x = countByMonths.index.tolist(),\n",
    "    y = countByMonths.values.tolist(),\n",
    "    hue= countByMonths.index.tolist(),\n",
    "    palette=colorSeq,\n",
    "    legend=False\n",
    ")\n",
    "mat.title('Product Page Visits Per Month')\n",
    "mat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6949b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Grouping by revenue and months\"\"\"\n",
    "# This is for the red bar (Months)\n",
    "highMonthRevenue = specific.loc[\n",
    "    (specific['Month'] == 'Dec') | (specific['Month'] == 'Mar') | (specific['Month'] == 'May') | (specific['Month'] == 'Nov') # Getting specific months\n",
    "].groupby('Revenue')['ProductRelated'].count()\n",
    "\n",
    "# This is for the remaning months\n",
    "lowMonthRevenue = specific.loc[\n",
    "    (specific['Month'] != 'Dec') | (specific['Month'] != 'Mar') | (specific['Month'] != 'May') | (specific['Month'] != 'Nov') # Getting specific months\n",
    "].groupby('Revenue')['ProductRelated'].count()\n",
    "\n",
    "# Creating dataframe\n",
    "tempDataframe1 = pandas.DataFrame(\n",
    "    pandas.concat(\n",
    "        [highMonthRevenue.rename('High Month Revenue'),\n",
    "        lowMonthRevenue.rename('Low Month Revenue')],\n",
    "        axis=1)\n",
    ")\n",
    "\n",
    "tempDataframe1.index.name = 'Revenue'\n",
    "\n",
    "newDf = tempDataframe1.reset_index().melt(\n",
    "    id_vars='Revenue',             \n",
    "    var_name='MonthType',          \n",
    "    value_name='Count'       \n",
    ")\n",
    "\n",
    "mat.figure(figsize=(12, 5))\n",
    "\n",
    "seaborn.barplot(\n",
    "    x = newDf['Revenue'],\n",
    "    y = newDf['Count'],\n",
    "    hue = newDf['MonthType']\n",
    ")\n",
    "\n",
    "customAxis = [val for val in range(0, newDf['Count'].max(), 500)]\n",
    "mat.yticks(customAxis)\n",
    "\n",
    "mat.show()\n",
    "\n",
    "print(tempDataframe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b7d137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Months where we generated the revenue (Full data)\n",
    "RevenuebyMonth = data.loc[data['Revenue'] == True].groupby('Month')['Revenue'].count()\n",
    "\n",
    "fig, ax = mat.subplots(1, 1, figsize=(15, 5))\n",
    "\n",
    "bar = seaborn.barplot(\n",
    "    x=RevenuebyMonth.index.tolist(),\n",
    "    y = RevenuebyMonth.values.tolist(),\n",
    "    hue = RevenuebyMonth.values.tolist(),\n",
    "    palette='coolwarm',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Adding values to the plot ----> For better intution and interpretation\n",
    "for labelsToTheBar in bar.containers:\n",
    "    bar.bar_label(labelsToTheBar, fmt='%.0f', padding=3)\n",
    "\n",
    "ax.set_title('Revenue Generated Counts Per Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc32f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Way Changed!\n",
    "pointfinder = data.loc[\n",
    "    ((data['Month'] == 'Nov') | (data['Month'] == 'Dec') | (data['Month'] == 'Mar') | (data['Month'] == 'May')) & (data['Revenue'] == True)\n",
    "]\n",
    "\n",
    "nonrevenuecustomer = data.loc[\n",
    "    ~(((data['Month'] == 'Nov') | (data['Month'] == 'Dec') | (data['Month'] == 'Mar') | (data['Month'] == 'May')) & (data['Revenue'] == True))\n",
    "]\n",
    "\n",
    "pointfinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buyers and non-buyers behaviour -> Comparison\n",
    "\n",
    "# Where the revenue was generated\n",
    "newVisitor = pointfinder.loc[pointfinder['VisitorType'] == 'New_Visitor']\n",
    "existingVisitor = pointfinder.loc[pointfinder['VisitorType'] == 'Returning_Visitor']\n",
    "\n",
    "fig, ax = mat.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "seaborn.scatterplot(x = newVisitor['ProductRelated'], y = newVisitor['ProductRelated_Duration'], hue = newVisitor['VisitorType'], ax = ax[0, 0])\n",
    "seaborn.scatterplot(x = existingVisitor['ProductRelated'], y = existingVisitor['ProductRelated_Duration'], hue = existingVisitor['VisitorType'], ax = ax[0, 1])\n",
    "\n",
    "ax[0, 0].set_title('New Visitors (Revenue Generated)')\n",
    "ax[0, 1].set_title('Existing Visitors (Revenue Generated)')\n",
    "\n",
    "# Where the revenue was not generated\n",
    "newVisitornon = nonrevenuecustomer.loc[nonrevenuecustomer['VisitorType'] == 'New_Visitor']\n",
    "existingVisitornon = nonrevenuecustomer.loc[nonrevenuecustomer['VisitorType'] == 'Returning_Visitor']\n",
    "\n",
    "seaborn.scatterplot(x = newVisitornon['ProductRelated'], y = newVisitornon['ProductRelated_Duration'], hue = newVisitornon['VisitorType'], ax = ax[1, 0])\n",
    "seaborn.scatterplot(x = existingVisitornon['ProductRelated'], y = existingVisitornon['ProductRelated_Duration'], hue = existingVisitornon['VisitorType'], ax = ax[1, 1])\n",
    "\n",
    "ax[1, 0].set_title('New Visitors (Revenue Not Generated)')\n",
    "ax[1, 1].set_title('Existing Visitors (Revenue Not Generated)')\n",
    "\n",
    "mat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e9d4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Canvas Figure\n",
    "fig, ax = mat.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "# Insights on whether the new visitors are buyers or non-buyers\n",
    "new_visitor_nonbuyer = data.loc[(data['VisitorType'] == 'New_Visitor') & (data['Revenue'] == False)]\n",
    "new_visitor_buyer = data.loc[(data['VisitorType'] == 'New_Visitor') & (data['Revenue'] == True)]\n",
    "\n",
    "# Plotting the counts --> New Visitors \n",
    "newVisitorCountData = [len(new_visitor_buyer), len(new_visitor_nonbuyer)] \n",
    "countPlot = seaborn.barplot(data = newVisitorCountData ,ax=ax[0])\n",
    "countPlot.set_title('New Visitor Buyer vs Non-Buyer Counts')\n",
    "countPlot.set_xticks([0, 1])\n",
    "countPlot.set_xticklabels(['Buyer', 'Non-Buyer'])\n",
    "\n",
    "# Adding numbers to the bars\n",
    "for thelabels in countPlot.containers:\n",
    "    countPlot.bar_label(thelabels, fmt='%.0f')\n",
    "\n",
    "# Insights on whether the Existing purchased or not\n",
    "exist_visitor_nonbuyer = data.loc[(data['VisitorType'] == 'Returning_Visitor') & (data['Revenue'] == False)]\n",
    "exist_visitor_buyer = data.loc[(data['VisitorType'] == 'Returning_Visitor') & (data['Revenue'] == True)]\n",
    "\n",
    "# Plotting the counts --> Existing Visitors\n",
    "existing = [len(exist_visitor_buyer), len(exist_visitor_nonbuyer)]\n",
    "colors = ['red', 'yellow'] \n",
    "countExisting = seaborn.barplot(data = existing, ax=ax[1])\n",
    "countExisting.set_title('Existing Visitor Buyer vs Non-Buyer Counts')\n",
    "countExisting.set_xticks([0, 1])\n",
    "countExisting.set_xticklabels(['Buyer', 'Non-Buyer'])\n",
    "for ExistingCountLabels in countExisting.containers:\n",
    "    countExisting.bar_label(ExistingCountLabels, fmt='%.0f', padding=1)\n",
    "mat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ad7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe04db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "badRetention = data.loc[(data['Revenue'] == False) & (data['VisitorType'] == 'Returning_Visitor')]\n",
    "goodRetention = data.loc[(data['Revenue'] == True) & (data['VisitorType'] == 'Returning_Visitor')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45b9fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to be used in the plots\n",
    "def apply_auto(array):\n",
    "\n",
    "    # Plot settings -> For Bad Retention\n",
    "    colors = []\n",
    "    system = []\n",
    "    \n",
    "    # Subtraction is also hardcoded!\n",
    "    for val in range(len(array) - 1): \n",
    "        if val == 0 or val == 1: # Values are hardcoded!\n",
    "            colors.append('darkblue')\n",
    "            system.append('Desktop')\n",
    "        elif val == 3 or val == 4:\n",
    "            colors.append('blue')\n",
    "            system.append('Mobile')\n",
    "        else:\n",
    "            colors.append('skyblue')\n",
    "            system.append('Tab or Book')\n",
    "    \n",
    "    return colors, system\n",
    "\n",
    "\n",
    "# MONTH\n",
    "# Exploring Page Value -> Who Purchased Something\n",
    "fig, ax = mat.subplots(3, 2, figsize=(20, 15))\n",
    "\n",
    "month_to_pagevalue = badRetention.groupby('Month')['PageValues'].mean().sort_values(ascending=False)\n",
    "month_to_pagevalue_good = goodRetention.groupby('Month')['PageValues'].mean().sort_values(ascending=False)\n",
    "pagevalue_mean = goodRetention.groupby('Month')['PageValues'].mean().sort_values(ascending=False).mean()\n",
    "\n",
    "seaborn.barplot(\n",
    "    x = month_to_pagevalue.index.tolist(),\n",
    "    y = month_to_pagevalue.values.tolist(),\n",
    "    ax=ax[0, 0]\n",
    ")\n",
    "ax[0, 0].axhline(y=pagevalue_mean, color='red', linestyle='--', label='Average Page Value', linewidth=2)\n",
    "ax[0, 0].set_title('Page Value by Month (Bad Retention)')\n",
    "ax[0, 0].set_xlabel('Month')\n",
    "ax[0, 0].set_ylabel('Average Page Value')\n",
    "ax[0, 0].legend()\n",
    "\n",
    "# Page Value and its relation with total pages viewed\n",
    "seaborn.barplot(\n",
    "    x = month_to_pagevalue_good.index.tolist(), \n",
    "    y = month_to_pagevalue_good.values.tolist(), \n",
    "    hue = month_to_pagevalue_good.index.tolist(), \n",
    "    ax=ax[0, 1]\n",
    ")\n",
    "\n",
    "\n",
    "# Operating Systems\n",
    "# Operating System and its relation with page value\n",
    "os_pagevalue = badRetention.groupby('OperatingSystems')['PageValues'].sum().sort_values(ascending=False)\n",
    "os_pagevalue_good = goodRetention.groupby('OperatingSystems')['PageValues'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Converting the values to text\n",
    "os_map = {\n",
    "    2: 'Mac OS',\n",
    "    3: 'Linux',\n",
    "    4: 'Chrome OS',\n",
    "    5: 'Android',\n",
    "    6: 'iOS',\n",
    "    7: 'Other',\n",
    "    8: 'Embedded'\n",
    "} \n",
    "\n",
    "os_pagevalue.index = os_pagevalue.index.map(os_map) # Value Changed\n",
    "clr1, syst1 = apply_auto(os_pagevalue) # Colors and System Type\n",
    "\n",
    "os_pagevalue_good.index = os_pagevalue_good.index.map(os_map) # Value Changed\n",
    "clr2, syst2 = apply_auto(os_pagevalue_good) # Colors and System Type\n",
    "\n",
    "seaborn.barplot(\n",
    "    x = os_pagevalue.index.tolist(),\n",
    "    y = os_pagevalue.values.tolist(),\n",
    "    ax=ax[1, 0],\n",
    "    hue = os_pagevalue.index.tolist(),\n",
    "    palette=clr1\n",
    ")\n",
    "\n",
    "ax[1, 0].set_title('Page Value by Operating System (Bad Retention)')\n",
    "ax[1, 0].set_xlabel('Operating System')\n",
    "ax[1, 0].set_ylabel('Total Page Value')\n",
    "ax[1, 0].legend(title='System Type', labels=syst1)\n",
    "\n",
    "seaborn.barplot(\n",
    "    x = os_pagevalue_good.index.tolist(),\n",
    "    y = os_pagevalue_good.values.tolist(),\n",
    "    ax=ax[1, 1],\n",
    "    hue = os_pagevalue_good.index.tolist(),\n",
    "    palette=clr2\n",
    ")\n",
    "ax[1, 1].set_title('Page Value by Operating System (Good Retention)')\n",
    "ax[1, 1].set_xlabel('Operating System')\n",
    "ax[1, 1].set_ylabel('Total Page Value')\n",
    "ax[1, 1].legend(title='System Type', labels=syst2)\n",
    "\n",
    "\n",
    "# Browsers\n",
    "browser_pagevalue = badRetention.groupby('Browser')['PageValues'].sum().sort_values(ascending=False)\n",
    "browser_pagevalue_good = goodRetention.groupby('Browser')['PageValues'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Updating the values\n",
    "browser_mapping = {\n",
    "    1: 'Chrome',\n",
    "    2: 'Safari',\n",
    "    3: 'Firefox',\n",
    "    4: 'Edge',\n",
    "    5: 'Internet Explorer',\n",
    "    6: 'Opera',\n",
    "    7: 'Samsung Internet',\n",
    "    8: 'UC Browser',\n",
    "    9: 'Android Browser',\n",
    "    10: 'Brave',\n",
    "    11: 'Vivaldi',\n",
    "    12: 'Maxthon',\n",
    "    13: 'Other / Unknown'\n",
    "}\n",
    "\n",
    "browser_pagevalue.index = browser_pagevalue.index.map(browser_mapping) \n",
    "browser_pagevalue_good.index = browser_pagevalue_good.index.map(browser_mapping)\n",
    "\n",
    "badBrowser = seaborn.barplot(\n",
    "    x = browser_pagevalue.index.tolist(),\n",
    "    y = browser_pagevalue.values.tolist(),\n",
    "    ax = ax[2, 0],\n",
    "    hue = browser_pagevalue.index.tolist()\n",
    ")\n",
    "for labels in badBrowser.containers:\n",
    "    badBrowser.bar_label(labels, fmt='%.0f', padding=1)\n",
    "ax[2, 0].set_title('Pagevalues by Browser (Bad Retention)')\n",
    "ax[2, 0].set_xlabel('Browsers')\n",
    "ax[2, 0].set_ylabel('Frequency')\n",
    "\n",
    "\n",
    "# Good Retention Browser based page values\n",
    "goodBrowser = seaborn.barplot(\n",
    "    x = browser_pagevalue_good.index.tolist(),\n",
    "    y = browser_pagevalue_good.values.tolist(),\n",
    "    ax = ax[2, 1],\n",
    "    hue = browser_pagevalue_good.index.tolist()\n",
    ")\n",
    "for labels in goodBrowser.containers:\n",
    "    goodBrowser.bar_label(labels, fmt='%.0f', padding=1)\n",
    "ax[2, 1].set_title('Pagevalues by Browser (Good Retention)')\n",
    "ax[2, 1].set_xlabel('Browsers')\n",
    "ax[2, 1].set_ylabel('Frequency')\n",
    "\n",
    "mat.tight_layout()\n",
    "mat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9160ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965723fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekend and Revenue\n",
    "revenueOnWeekend = data.loc[(data['Weekend'] == True) & (data['Revenue'] == True)]\n",
    "existing = revenueOnWeekend.loc[revenueOnWeekend['VisitorType'] == 'Returning_Visitor']\n",
    "newCustomer = revenueOnWeekend.loc[revenueOnWeekend['VisitorType'] == 'New_Visitor']\n",
    "print('Existing Customer Length -', len(existing))\n",
    "print('Existing Customer revenue (PageValue)', existing['PageValues'].sum())\n",
    "print('New Customer Length -', len(newCustomer))\n",
    "print('New Customer revenue (PageValue)', newCustomer['PageValues'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372883eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Weekend and Revenue\n",
    "revenueOnNoWeekend = data.loc[(data['Weekend'] == False) & (data['Revenue'] == True)]\n",
    "\n",
    "existing = revenueOnNoWeekend.loc[revenueOnNoWeekend['VisitorType'] == 'Returning_Visitor']\n",
    "newCustomer = revenueOnNoWeekend.loc[revenueOnNoWeekend['VisitorType'] == 'New_Visitor']\n",
    "print('Existing Customer Length -', len(existing))\n",
    "print('Existing Customer revenue (PageValue)', existing['PageValues'].sum())\n",
    "print('New Customer Length -', len(newCustomer))\n",
    "print('New Customer revenue (PageValue)', newCustomer['PageValues'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_training",
   "metadata": {},
   "source": [
    "## Model Training and Testing\n",
    "We will train and test several machine learning models to predict customer sentiment and revenue. The models we will use include:\n",
    "* Linear Regression\n",
    "* Polynomial Regression\n",
    "* Random Forest Regressor\n",
    "* Gradient Boosting Regressor\n",
    "\n",
    "We will evaluate the performance of these models using metrics like RMSE and R-squared to determine the most accurate model for our business needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f102f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machin learning Libraries\n",
    "\n",
    "# Preprocessing Methods\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Importing Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Machine learning Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score, root_mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d261ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_Xtrain(data_var):\n",
    "\n",
    "    # Saving encoders for specific columns\n",
    "    lbEncoder = {}\n",
    "    scEncoder = {}\n",
    "    prEncoder = {}\n",
    "\n",
    "    if not data_var.empty:\n",
    "\n",
    "        catData = data_var.select_dtypes(include=['bool', 'object'])\n",
    "        numData = data_var.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "        # First Categorical Data\n",
    "        catCol = catData.columns\n",
    "\n",
    "        for eachColumn in catCol:\n",
    "\n",
    "            # Fitting the method\n",
    "            encoder = LabelEncoder()\n",
    "            new_Column_Value = encoder.fit_transform(catData[eachColumn])\n",
    "            lbEncoder[eachColumn] = encoder\n",
    "\n",
    "            # Scaling the encoded ones\n",
    "            scaler = StandardScaler()\n",
    "            scaled_New_values = scaler.fit_transform(pandas.DataFrame(new_Column_Value, columns=[eachColumn]))\n",
    "            scEncoder[eachColumn] = scaler\n",
    "\n",
    "            # Passing it to the respective column\n",
    "            catData[eachColumn] = scaled_New_values.flatten()\n",
    "\n",
    "            # Transferring each transformers\n",
    "\n",
    "        # Second Continous Data\n",
    "        numCol = numData.columns\n",
    "\n",
    "        for eachColumns in numCol:\n",
    "\n",
    "            # Power Transformer\n",
    "            columnMean = numData[eachColumns].mean()\n",
    "            columnMedian = numData[eachColumns].median()\n",
    "\n",
    "            # This transformer handles the skewness of the data as well as\n",
    "            # It scales down the columns. So no need to use any transformation method\n",
    "            if columnMean > columnMedian:\n",
    "                \n",
    "                # When the data is right skewed\n",
    "                transformer = PowerTransformer(method='yeo-johnson')\n",
    "                numScaledValues = transformer.fit_transform(pandas.DataFrame(numData[eachColumns], columns=[eachColumns]))\n",
    "                numData[eachColumns] = numScaledValues\n",
    "            \n",
    "            elif columnMean < columnMedian:\n",
    "\n",
    "                # When the data is left skewed\n",
    "                transformer = PowerTransformer(method='yeo-johnson')\n",
    "                numScaledValues = transformer.fit_transform(pandas.DataFrame(numData[eachColumns], columns=[eachColumns]))\n",
    "                numData[eachColumns] = numScaledValues\n",
    "            \n",
    "\n",
    "            # Transferring the transformer\n",
    "            prEncoder[eachColumns] = transformer\n",
    "\n",
    "        # Concatenating both the data\n",
    "        concat_data = pandas.DataFrame(\n",
    "            pandas.concat([catData, numData], axis=1)\n",
    "        )\n",
    "\n",
    "        return concat_data, lbEncoder, scEncoder, prEncoder\n",
    "\n",
    "    else:\n",
    "        return 'please pass the data in this function'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5335fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second function for X test\n",
    "def convert_Xtest(data_var, en1, en2, en3):\n",
    "    \n",
    "    # Checking the variable has something\n",
    "    if not data_var.empty and any(x is not None for x in (en1, en2, en3)):\n",
    "        \n",
    "        for col, label in en1.items():\n",
    "            data_var[col] = label.transform(data_var[col])\n",
    "        \n",
    "        for col, scaler in en2.items():\n",
    "            data_var[col] = scaler.transform(data_var[[col]])\n",
    "    \n",
    "        for col, transform in en3.items():\n",
    "            data_var[col] = transform.transform(data_var[[col]])\n",
    "    \n",
    "        return data_var\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Compare the RMSE\n",
    "def rangeCheck(yTest, prediction):\n",
    "    target_range = numpy.max(yTest) - numpy.min(yTest)\n",
    "    rmse_test = numpy.sqrt(numpy.mean((yTest - prediction) ** 2))\n",
    "    rmse_percentage = (rmse_test / target_range) * 100\n",
    "\n",
    "    print(\"Target Range:\", target_range)\n",
    "    print(\"Test RMSE:\", rmse_test)\n",
    "    print(\"RMSE as % of Target Range:\", rmse_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6eb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "# X and Y initialization\n",
    "x = data.drop(columns=['PageValues'])\n",
    "y = data['PageValues']\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, ry_train, ry_test = train_test_split(x, y, test_size=0.2, random_state=57)\n",
    "\n",
    "# X_training preparations\n",
    "X_train, encoded, scaled, transformed = convert_Xtrain(X_train)\n",
    "x_transfromed_columns = list(X_train.columns)\n",
    "X_test = convert_Xtest(X_test, encoded, scaled, transformed)[x_transfromed_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the y_train and y_test both to check the model performance\n",
    "y_train = numpy.log1p(ry_train)\n",
    "y_test = numpy.log1p(ry_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78169039",
   "metadata": {},
   "source": [
    "### Linear Model Training and Prediction with Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004ae05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "linearModel = LinearRegression()\n",
    "linearModel.fit(X_train, y_train)\n",
    "\n",
    "# Training data fit\n",
    "trainedPrediction = linearModel.predict(X_train)\n",
    "train_rmse = root_mean_squared_error(y_train, trainedPrediction)\n",
    "train_r2 = r2_score(y_train, trainedPrediction)\n",
    "\n",
    "# Test data\n",
    "predict = linearModel.predict(X_test)\n",
    "test_rmse = root_mean_squared_error(y_test, predict)\n",
    "test_r2 = r2_score(y_test, predict)\n",
    "\n",
    "print('Trained Prediction Statistics')\n",
    "print('Train RMSE Score - ', train_rmse)\n",
    "print('Train R2 Score - ', train_r2, '\\n')\n",
    "\n",
    "print('Test Prediction Statistics')\n",
    "print('Test RMSE - ', test_rmse)\n",
    "print('Test r2 - ', test_r2, '\\n')\n",
    "\n",
    "# RMSE Comparison\n",
    "print('RMSE Comparison')\n",
    "rangeCheck(y_test, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcd1524",
   "metadata": {},
   "source": [
    "### Polynomial Regression with evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93d4b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree Initialization\n",
    "degree = [1, 2, 3]\n",
    "\n",
    "# Score Collectors\n",
    "score = {}\n",
    "\n",
    "# Y_prediction Variable for plotting\n",
    "y_pred = {}\n",
    "\n",
    "for eachDegree in degree:\n",
    "    \n",
    "    # Training the model\n",
    "    poly = PolynomialFeatures(degree=eachDegree, include_bias=False)\n",
    "    x_train_poly = poly.fit_transform(X_train)\n",
    "    x_test_poly = poly.transform(X_test)\n",
    "\n",
    "    poly_reg = LinearRegression()\n",
    "    poly_reg.fit(x_train_poly, y_train)\n",
    "\n",
    "    # Training data fit\n",
    "    p_train_pred = poly_reg.predict(x_train_poly)\n",
    "    p_train_rmse = root_mean_squared_error(y_train, p_train_pred)\n",
    "    p_train_r2 = r2_score(y_train, p_train_pred)\n",
    "\n",
    "    # Test data\n",
    "    p_predict = poly_reg.predict(x_test_poly)\n",
    "    p_test_rmse = root_mean_squared_error(y_test, p_predict)\n",
    "    p_test_r2 = r2_score(y_test, p_predict)\n",
    "\n",
    "    print('Training set')\n",
    "    print('training RMSE - ', p_train_rmse)\n",
    "    print('training R2 - ', p_train_r2)\n",
    "\n",
    "    print()\n",
    "    print('Testing RMSE - ', p_test_rmse)\n",
    "    print('Testing R2 - ', p_test_r2)\n",
    "\n",
    "    print('Testing Set')\n",
    "\n",
    "    # Sending the score for each degree\n",
    "    score[eachDegree] = {\n",
    "        'train': [p_train_rmse, p_train_r2],\n",
    "        'test': [p_test_rmse, p_test_r2]\n",
    "    }\n",
    "\n",
    "    # Transferring the variables\n",
    "    y_pred[f'{eachDegree}'] = p_predict\n",
    "\n",
    "    rangeCheck(y_test, p_predict)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0edcd4",
   "metadata": {},
   "source": [
    "### RANSAC (Random Sample Concensus) Training and Testing with model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e466c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals Finder\n",
    "def findResidual(model, x_train, x_test, y_train, y_test):\n",
    "\n",
    "    if any(x is not None for x in (model, x_train, x_test, y_train, y_test)):\n",
    "\n",
    "        # fitting the training set\n",
    "        model.fit(x_train, y_train)\n",
    "\n",
    "        # Predicting Training and testing \n",
    "        xtrainPredict = model.predict(x_train)\n",
    "        xtestPredict = model.predict(x_test)\n",
    "\n",
    "        # Redisuals Calculations\n",
    "        trainResiduals = numpy.abs(y_train - xtrainPredict)\n",
    "        testRedisuals = numpy.abs(y_test - xtestPredict)\n",
    "    \n",
    "        return trainResiduals, testRedisuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40de50f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the model and Model training\n",
    "\n",
    "# Parameters for the Random Sample Consencus\n",
    "base_model = [LinearRegression(), Lasso()]\n",
    "modelNames = ['Linear Model', 'L1 Regularization or Lasso']\n",
    "\n",
    "# Residual Threhsold for training and testing\n",
    "count = 0\n",
    "dataLen = len(data)\n",
    "\n",
    "inlinearAndOutlier = {}\n",
    "\n",
    "# Y_pred score\n",
    "lassoPredict = []\n",
    "\n",
    "for model in base_model:\n",
    "\n",
    "    xtrain_residualThreshold, xtest_residualThreshold = findResidual(\n",
    "            model, X_train, X_test, y_train, y_test\n",
    "        )\n",
    "    \n",
    "    xtrain_percentile =  numpy.percentile(xtrain_residualThreshold, 90)\n",
    "    xtest_percentile = numpy.percentile(xtest_residualThreshold, 90)\n",
    "    \n",
    "    randomSampleConsencus = RANSACRegressor(\n",
    "        estimator=model,\n",
    "        min_samples=0.5,\n",
    "        residual_threshold=xtrain_percentile,\n",
    "        max_trials = 10,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    randomSampleConsencus.fit(X_train, y_train)\n",
    "\n",
    "    # Training Set and Its Score\n",
    "    rm_train_pred = randomSampleConsencus.predict(X_train)\n",
    "    rm_rmse = root_mean_squared_error(y_train, rm_train_pred)\n",
    "    rm_r2 = r2_score(y_train, rm_train_pred)\n",
    "\n",
    "    # Testing Set and Its score\n",
    "    rm_test_pred = randomSampleConsencus.predict(X_test)\n",
    "    rm_test_rmse = root_mean_squared_error(y_test, rm_test_pred)\n",
    "    rm_test_r2 = r2_score(y_test, rm_test_pred)\n",
    "\n",
    "    print(modelNames[count], '\\n')\n",
    "    print('Trained Prediction Statistics')\n",
    "    print('Train RMSE Score - ', rm_rmse)\n",
    "    print('Train R2 Score - ', rm_r2, '\\n')\n",
    "\n",
    "    print('Test Prediction Statistics')\n",
    "    print('Test RMSE - ', rm_test_rmse)\n",
    "    print('Test r2 - ', rm_test_r2, '\\n')\n",
    "\n",
    "    # Transferring the values\n",
    "    inlineMask = randomSampleConsencus.inlier_mask_\n",
    "    outlierMask = numpy.logical_not(inlineMask)\n",
    "\n",
    "    inlinearAndOutlier[modelNames[count]] = {\n",
    "        'inline_mask': inlineMask,\n",
    "        'outline_mask': outlierMask\n",
    "    }\n",
    "\n",
    "    if modelNames[count] == 'L1 Regularization or Lasso':\n",
    "        lassoPredict.append(rm_test_pred)\n",
    "\n",
    "    # Range Check Comparison\n",
    "    rangeCheck(y_test, rm_test_pred)\n",
    "\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07ea481",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the model\n",
    "randomForest = RandomForestRegressor(\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=26,\n",
    "    min_samples_split=17,\n",
    "    max_features='sqrt',\n",
    "    n_estimators=453,\n",
    "    random_state=42,\n",
    "    warm_start=True\n",
    ")\n",
    "\n",
    "randomForest.fit(X_train, y_train)\n",
    "\n",
    "# Training Evaluation\n",
    "train_predict = randomForest.predict(X_train)\n",
    "trainrmse = root_mean_squared_error(y_train, train_predict)\n",
    "trainr2 = r2_score(y_train, train_predict)\n",
    "\n",
    "# Testing Evaluation\n",
    "test_predict = randomForest.predict(X_test)\n",
    "testrmse = root_mean_squared_error(y_test, test_predict)\n",
    "testr2 = r2_score(y_test, test_predict)\n",
    "\n",
    "print('Training Scores')\n",
    "print('RMSE - ', trainrmse)\n",
    "print('R2 Score - ', trainr2, '\\n')\n",
    "\n",
    "print('Test Score')\n",
    "print('RMSE - ', testrmse)\n",
    "print('R2 Score - ', testr2)\n",
    "print()\n",
    "rangeCheck(y_test, test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fae23b",
   "metadata": {},
   "source": [
    "### Gradient Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the model for the training and testing\n",
    "gradientReg = GradientBoostingRegressor(\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=13,\n",
    "    min_samples_split=5,\n",
    "    max_features='sqrt',\n",
    "    n_estimators=313,\n",
    "    random_state=42,\n",
    "    warm_start=True,\n",
    "    learning_rate=0.01\n",
    ")\n",
    "gradientReg.fit(X_train, y_train)\n",
    "\n",
    "# Training Evaluation\n",
    "gtrain_predict = gradientReg.predict(X_train)\n",
    "gtrainrmse = root_mean_squared_error(y_train, gtrain_predict)\n",
    "gtrainr2 = r2_score(y_train, gtrain_predict)\n",
    "\n",
    "# Testing Evaluation\n",
    "gtest_predict = gradientReg.predict(X_test)\n",
    "gtestrmse = root_mean_squared_error(y_test, gtest_predict)\n",
    "gtestr2 = r2_score(y_test, gtest_predict)\n",
    "\n",
    "print('Training Scores')\n",
    "print('RMSE - ', gtrainrmse)\n",
    "print('R2 Score - ', gtrainr2, '\\n')\n",
    "\n",
    "print('Test Score')\n",
    "print('RMSE - ', gtestrmse)\n",
    "print('R2 Score - ', gtestr2)\n",
    "\n",
    "print()\n",
    "rangeCheck(y_test, gtest_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aeb239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the variance\n",
    "variance = numpy.var(y_train)\n",
    "print('The Variance - ', variance)\n",
    "\n",
    "# Visualizing the variance\n",
    "mat.hist(y_train, bins=20, edgecolor='k')\n",
    "mat.title('Target Variable Distribution')\n",
    "mat.xlabel('Target Vairable')\n",
    "mat.ylabel('Frequency')\n",
    "mat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98224ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(xtrain, xtest, ytest, ytrain):\n",
    "\n",
    "    # Linear Model\n",
    "    # Training the model\n",
    "    linearModel = LinearRegression()\n",
    "    linearModel.fit(xtrain, ytrain)\n",
    "\n",
    "    # Training data fit\n",
    "    trainedPrediction = linearModel.predict(xtrain)\n",
    "    train_rmse = root_mean_squared_error(ytrain, trainedPrediction)\n",
    "    train_r2 = r2_score(ytrain, trainedPrediction)\n",
    "\n",
    "    # Test data\n",
    "    predict = linearModel.predict(xtest)\n",
    "    test_rmse = root_mean_squared_error(ytest, predict)\n",
    "    test_r2 = r2_score(ytest, predict)\n",
    "\n",
    "    # Polynomial Regression\n",
    "    # Degree Initialization\n",
    "    degree = [1, 2, 3]\n",
    "    \n",
    "    # Score Collectors\n",
    "    score = {}\n",
    "\n",
    "    # Y_prediction Variable for plotting\n",
    "    y_pred = {}\n",
    "\n",
    "    for eachDegree in degree:\n",
    "        \n",
    "        # Training the model\n",
    "        poly = PolynomialFeatures(degree=eachDegree, include_bias=False)\n",
    "        x_train_poly = poly.fit_transform(xtrain)\n",
    "        x_test_poly = poly.transform(xtest)\n",
    "\n",
    "        poly_reg = LinearRegression()\n",
    "        poly_reg.fit(x_train_poly, ytrain)\n",
    "\n",
    "        # Training data fit\n",
    "        p_train_pred = poly_reg.predict(x_train_poly)\n",
    "        p_train_rmse = root_mean_squared_error(ytrain, p_train_pred)\n",
    "        p_train_r2 = r2_score(ytrain, p_train_pred)\n",
    "\n",
    "        # Test data\n",
    "        p_predict = poly_reg.predict(x_test_poly)\n",
    "        p_test_rmse = root_mean_squared_error(ytest, p_predict)\n",
    "        p_test_r2 = r2_score(ytest, p_predict)\n",
    "\n",
    "        # Transferring the variables\n",
    "        y_pred[f'{eachDegree}'] = [p_test_rmse, p_train_rmse]\n",
    "\n",
    "    # Random Sample Conusensus\n",
    "    # Parameters for the Random Sample Consencus\n",
    "    base_model = [LinearRegression(), Lasso()]\n",
    "    modelNames = ['Linear Model', 'L1 Regularization or Lasso']\n",
    "\n",
    "    # Residual Threhsold for training and testing\n",
    "    count = 0\n",
    "\n",
    "    inlinearAndOutlier = {}\n",
    "\n",
    "    # Y_pred score\n",
    "    lassoPredict = []\n",
    "\n",
    "    for model in base_model:\n",
    "\n",
    "        xtrain_residualThreshold, xtest_residualThreshold = findResidual(\n",
    "                model, xtrain, xtest, ytrain, ytest\n",
    "            )\n",
    "        \n",
    "        xtrain_percentile =  numpy.percentile(xtrain_residualThreshold, 90)\n",
    "        xtest_percentile = numpy.percentile(xtest_residualThreshold, 90)\n",
    "        \n",
    "        randomSampleConsencus = RANSACRegressor(\n",
    "            estimator=model,\n",
    "            min_samples=0.5,\n",
    "            residual_threshold=xtrain_percentile,\n",
    "            max_trials = 10,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        randomSampleConsencus.fit(xtrain, ytrain)\n",
    "\n",
    "        # Training Set and Its Score\n",
    "        rm_train_pred = randomSampleConsencus.predict(xtrain)\n",
    "        rm_rmse = root_mean_squared_error(ytrain, rm_train_pred)\n",
    "        rm_r2 = r2_score(ytrain, rm_train_pred)\n",
    "\n",
    "        # Testing Set and Its score\n",
    "        rm_test_pred = randomSampleConsencus.predict(xtest)\n",
    "        rm_test_rmse = root_mean_squared_error(ytest, rm_test_pred)\n",
    "        rm_test_r2 = r2_score(ytest, rm_test_pred)\n",
    "\n",
    "        # Transferring the values\n",
    "        inlineMask = randomSampleConsencus.inlier_mask_\n",
    "        outlierMask = numpy.logical_not(inlineMask)\n",
    "\n",
    "        inlinearAndOutlier[modelNames[count]] = {\n",
    "            'inline_mask': inlineMask,\n",
    "            'outline_mask': outlierMask\n",
    "        }\n",
    "\n",
    "        if modelNames[count] == 'L1 Regularization or Lasso':\n",
    "            lassoPredict.append([rm_test_rmse, rm_rmse])\n",
    "\n",
    "        # Range Check Comparison\n",
    "        rangeCheck(ytest, rm_test_pred)\n",
    "\n",
    "        count += 1\n",
    "    \n",
    "    # Random Forest Regression\n",
    "    \n",
    "    # Gradient Boost\n",
    "    # Preparing the model for the training and testing\n",
    "    gradientReg = GradientBoostingRegressor(\n",
    "        max_depth=3,\n",
    "        min_samples_leaf=13,\n",
    "        min_samples_split=5,\n",
    "        max_features='sqrt',\n",
    "        n_estimators=313,\n",
    "        random_state=42,\n",
    "        warm_start=True,\n",
    "        learning_rate=0.01\n",
    "    )\n",
    "    gradientReg.fit(xtrain, ytrain)\n",
    "\n",
    "    # Training Evaluation\n",
    "    gtrain_predict = gradientReg.predict(xtrain)\n",
    "    gtrainrmse = root_mean_squared_error(ytrain, gtrain_predict)\n",
    "    gtrainr2 = r2_score(ytrain, gtrain_predict)\n",
    "\n",
    "    # Testing Evaluation\n",
    "    gtest_predict = gradientReg.predict(xtest)\n",
    "    gtestrmse = root_mean_squared_error(ytest, gtest_predict)\n",
    "    gtestr2 = r2_score(ytest, gtest_predict)\n",
    "\n",
    "    rangeCheck(ytest, gtest_predict)\n",
    "\n",
    "    # Random Forest Regression\n",
    "    # Preparing the model\n",
    "    randomForest = RandomForestRegressor(\n",
    "        max_depth=5,\n",
    "        min_samples_leaf=26,\n",
    "        min_samples_split=17,\n",
    "        max_features='sqrt',\n",
    "        n_estimators=453,\n",
    "        random_state=42,\n",
    "        warm_start=True\n",
    "    )\n",
    "\n",
    "    randomForest.fit(xtrain, ytrain)\n",
    "\n",
    "    # Training Evaluation\n",
    "    train_predict = randomForest.predict(xtrain)\n",
    "    trainrmse = root_mean_squared_error(ytrain, train_predict)\n",
    "    trainr2 = r2_score(ytrain, train_predict)\n",
    "\n",
    "    # Testing Evaluation\n",
    "    test_predict = randomForest.predict(xtest)\n",
    "    testrmse = root_mean_squared_error(ytest, test_predict)\n",
    "    testr2 = r2_score(ytest, test_predict)\n",
    "\n",
    "    dict2 = {\n",
    "        'Linear Model': [test_rmse, train_rmse],\n",
    "        'Random Sample Concuses (Lasso Regularization)': lassoPredict[0],\n",
    "        'Polynomial Regression': y_pred['2'],\n",
    "        'Random Forest Regression': [testrmse, rm_rmse],\n",
    "        'Gradient Boost Regression': [gtestrmse, gtrainrmse]\n",
    "    }\n",
    "    \n",
    "\n",
    "    return dict2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb444234",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmseScore = train_model(X_train, X_test, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada46e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RMSE Score (Root Mean Squared Error)')\n",
    "training_test_score = pandas.DataFrame.from_dict(rmseScore, orient=\"index\", columns=['TestScore', 'TrainScore'])\n",
    "training_test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a795383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Compare it with the baseline of the dataset\n",
    "baseline = numpy.full_like(y_test, y_train.mean())\n",
    "baseline_rmse = float(numpy.sqrt(mean_squared_error(y_test, baseline)))\n",
    "\n",
    "# Checking this baseline rmse with the rmse score we are getting\n",
    "for key, values in compData['RMSE'].items():\n",
    "\n",
    "    if values >= baseline_rmse:\n",
    "        print(f'{key} underperforms the baseline Mean')\n",
    "    else:\n",
    "        print(f'{key} outperforms the baseline Mean,\\n\\nwhere, \\n{key} \\nbaseline RMSE - {baseline_rmse} > Model RMSE {values}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd4938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's perform scatter plot analysis for each model\n",
    "\n",
    "# Linear Model\n",
    "# Create figure with custom GridSpec layout\n",
    "fig = mat.figure(figsize=(15, 10))\n",
    "gs = fig.add_gridspec(2, 3)\n",
    "\n",
    "# Top row\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "# Bottom row: [1, 0] separate, [1, 1] and [1, 2] merged\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax5 = fig.add_subplot(gs[1, 1:])\n",
    "\n",
    "# Scatter plots\n",
    "ax1.scatter(y_test, predict)\n",
    "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "         color='red', linestyle='--', linewidth=2)\n",
    "ax1.set_title('Linear Model Scatter Plot')\n",
    "\n",
    "ax2.scatter(y_test, y_pred['2'])\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "         color='red', linestyle='--', linewidth=2)\n",
    "ax2.set_title('Polynomial Regression Scatter Plot')\n",
    "\n",
    "ax3.scatter(y_test, lassoPredict)\n",
    "ax3.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "         color='red', linestyle='--', linewidth=2)\n",
    "ax3.set_title('Lasso Regularization (L1) Model Scatter Plot')\n",
    "\n",
    "ax4.scatter(y_test, test_predict)\n",
    "ax4.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "         color='red', linestyle='--', linewidth=2)\n",
    "ax4.set_title('Random Forest Regressor Scatter Plot')\n",
    "\n",
    "ax5.scatter(y_test, gtest_predict)\n",
    "ax5.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()],\n",
    "         color='red', linestyle='--', linewidth=2)\n",
    "ax5.set_title('Gradient Boost Regressor Scatter Plot')\n",
    "\n",
    "mat.tight_layout()\n",
    "mat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This analysis provides a comprehensive overview of customer behavior on our e-commerce platform. By leveraging feature engineering and machine learning, we can predict customer sentiment and revenue with a high degree of accuracy. The insights gained from this analysis will help us make data-driven decisions to improve our platform and grow our business."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data env jptrbook",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
